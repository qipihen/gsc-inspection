import streamlit as st
import pandas as pd
from google.oauth2 import service_account
from googleapiclient.discovery import build
import time
import json

st.set_page_config(page_title="GSC URL Inspection æ‰¹é‡æ£€æµ‹", layout="wide")
st.title("ğŸ“Š GSC URL Inspection æ‰¹é‡æ£€æµ‹å·¥å…·ï¼ˆäº‘ç«¯ç‰ˆï¼‰")

uploaded_key = st.file_uploader("ä¸Šä¼  Google Service Account JSON å¯†é’¥æ–‡ä»¶", type=["json"])
uploaded_urls = st.file_uploader("ä¸Šä¼  URL åˆ—è¡¨ (.txt æˆ– .csv)", type=["txt", "csv"])
site_url = st.text_input("è¾“å…¥ GSC å±æ€§ç½‘å€ï¼ˆå¿…é¡»ä¸GSCé‡Œä¸€è‡´ï¼Œå¦‚ https://www.soulinconn.com/ï¼‰")

if uploaded_key and uploaded_urls and site_url:
    if st.button("ğŸš€ å¼€å§‹æ£€æµ‹"):
        # è¯»å–å¯†é’¥
        try:
            creds_dict = json.load(uploaded_key)
        except:
            st.error("âŒ æ— æ³•è§£æ JSON å¯†é’¥")
            st.stop()

        creds = service_account.Credentials.from_service_account_info(
            creds_dict, scopes=["https://www.googleapis.com/auth/webmasters"]
        )
        service = build('searchconsole', 'v1', credentials=creds)

        # è¯»å– URL åˆ—è¡¨
        if uploaded_urls.name.endswith(".csv"):
            df_urls = pd.read_csv(uploaded_urls, header=None)
            urls_to_check = df_urls.iloc[:, 0].dropna().tolist()
        else:
            urls_to_check = uploaded_urls.read().decode("utf-8").splitlines()
            urls_to_check = [u.strip() for u in urls_to_check if u.strip()]

        st.write(f"å…±åŠ è½½åˆ° **{len(urls_to_check)}** æ¡URLï¼Œå¼€å§‹æ£€æµ‹...")
        results = []
        progress_bar = st.progress(0)

        for idx, u in enumerate(urls_to_check, start=1):
            try:
                request = {
                    "inspectionUrl": u,
                    "siteUrl": site_url
                }
                result = service.urlInspection().index().inspect(body=request).execute()
                index_status = result['inspectionResult']['indexStatusResult']
                coverage = index_status.get('coverageState', 'UNKNOWN')
                verdict = index_status.get('verdict', 'UNKNOWN')
                last_crawl = index_status.get('lastCrawlTime', 'N/A')

                in_queue = "âœ…" if coverage.startswith("Discovered") else ""
                results.append({
                    "URL": u,
                    "æ”¶å½•çŠ¶æ€": coverage,
                    "åˆ¤æ–­ç»“æœ": verdict,
                    "æœ€åæŠ“å–æ—¶é—´": last_crawl,
                    "æ˜¯å¦æ’é˜Ÿä¸­": in_queue
                })
            except Exception as e:
                results.append({
                    "URL": u,
                    "æ”¶å½•çŠ¶æ€": "ERROR",
                    "åˆ¤æ–­ç»“æœ": str(e),
                    "æœ€åæŠ“å–æ—¶é—´": "",
                    "æ˜¯å¦æ’é˜Ÿä¸­": ""
                })

            progress_bar.progress(idx / len(urls_to_check))
            time.sleep(0.1)

        df_results = pd.DataFrame(results)
        st.success("âœ… æ£€æµ‹å®Œæˆ")
        st.dataframe(df_results)

        csv = df_results.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ CSV", data=csv, file_name="gsc_results.csv", mime="text/csv")
else:
    st.info("è¯·ä¸Šä¼ å¯†é’¥æ–‡ä»¶ + URL åˆ—è¡¨ï¼Œå¹¶è¾“å…¥ç«™ç‚¹URL")

import streamlit as st
import pandas as pd
from datetime import datetime
from google.oauth2 import service_account
from googleapiclient.discovery import build

# ------------------------- ä½ çš„ GSC æ£€æµ‹å‡½æ•° -------------------------
def inspect_url(sa_file, site_url, url):
    """è°ƒç”¨ Google Search Console API æ£€æµ‹ URL çŠ¶æ€"""
    SCOPES = ['https://www.googleapis.com/auth/webmasters']
    creds = service_account.Credentials.from_service_account_info(
        sa_file, scopes=SCOPES)
    service = build('searchconsole', 'v1', credentials=creds)

    try:
        request = {
            'inspectionUrl': url,
            'siteUrl': site_url
        }
        response = service.urlInspection().index().inspect(body=request).execute()
        status = response['inspectionResult']['indexStatusResult']['coverageState']
        return status
    except Exception as e:
        return f"Error: {e}"

# ------------------------- Streamlit åº”ç”¨ -------------------------
st.set_page_config(page_title="GSC URL æ‰¹é‡æ£€æµ‹å·¥å…·ï¼ˆäº‘ç«¯ç‰ˆï¼‰", layout="wide")
st.title("ğŸš€ GSC URL Inspection æ‰¹é‡æ£€æµ‹å·¥å…· (æ”¯æŒæ–­ç‚¹ç»­è·‘)")

# å­˜å‚¨è¿è¡Œç»“æœåœ¨ session_state
if "results" not in st.session_state:
    st.session_state.results = []

# ä¸Šä¼ å‡­è¯
uploaded_json = st.file_uploader("ğŸ“‚ ä¸Šä¼  Google Service Account JSON å¯†é’¥æ–‡ä»¶", type=["json"])

# ä¸Šä¼  URL åˆ—è¡¨ï¼ˆCSV æˆ– TXTï¼‰
uploaded_csv = st.file_uploader("ğŸ“‚ ä¸Šä¼  URL åˆ—è¡¨ æˆ– ä¸Šæ¬¡çš„è¿›åº¦æ–‡ä»¶", type=["csv", "txt"])

# è¾“å…¥ GSC å±æ€§
site_url = st.text_input("ğŸŒ GSC å±æ€§ç½‘å€ï¼ˆå¿…é¡»ä¸ GSC ä¸€è‡´ï¼‰", "https://www.example.com/")

# å¼€å§‹æ£€æµ‹
if st.button("ğŸš€ å¼€å§‹æ£€æµ‹"):
    if uploaded_json and uploaded_csv and site_url:
        sa_dict = pd.read_json(uploaded_json, typ='series').to_dict()

        # è¯»å–è¾“å…¥æ–‡ä»¶
        df_input = pd.read_csv(uploaded_csv, header=None)
        df_input.columns = ["url"] if df_input.shape[1] == 1 else df_input.columns

        # å¦‚æœæœ‰ status åˆ—ï¼Œè¯´æ˜æ˜¯ä¸Šæ¬¡è¿›åº¦ï¼Œè¿‡æ»¤å·²å®Œæˆçš„
        if "status" in df_input.columns:
            done_count = df_input["status"].notna().sum()
            st.info(f"æ£€æµ‹åˆ°å·²æœ‰ {done_count} æ¡ç»“æœï¼Œå°†è·³è¿‡è¿™äº› URL...")
            urls_to_check = df_input[df_input["status"].isna()]["url"].tolist()
            # å·²å®Œæˆçš„éƒ¨åˆ†åŠ è½½åˆ° results
            st.session_state.results = df_input[df_input["status"].notna()].to_dict("records")
        else:
            urls_to_check = df_input["url"].tolist()

        total_urls = len(urls_to_check)
        st.write(f"å…±éœ€æ£€æµ‹ {total_urls} æ¡ URL")

        progress_bar = st.progress(0)
        status_text = st.empty()

        for idx, url in enumerate(urls_to_check):
            status = inspect_url(sa_dict, site_url, url)
            st.session_state.results.append({
                "url": url,
                "status": status
            })

            progress_bar.progress((idx + 1) / total_urls)
            status_text.text(f"{idx+1}/{total_urls} å·²å®Œæˆ: {url} --> {status}")

            # å®æ—¶ç”Ÿæˆéƒ¨åˆ†ç»“æœæ–‡ä»¶ï¼ˆåŒ…å«å·²å®Œæˆçš„éƒ¨åˆ†ï¼‰
            temp_df = pd.DataFrame(st.session_state.results)
            csv_data = temp_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="â¬‡ å®æ—¶ä¸‹è½½å½“å‰è¿›åº¦CSVï¼ˆå¯ç”¨äºæ–­ç‚¹ç»­è·‘ï¼‰",
                data=csv_data,
                file_name='gsc_results_partial.csv',
                mime='text/csv'
            )

        st.success("âœ… æ£€æµ‹å®Œæˆï¼")
        final_df = pd.DataFrame(st.session_state.results)
        st.download_button(
            label="â¬‡ ä¸‹è½½æœ€ç»ˆå®Œæ•´ç»“æœCSV",
            data=final_df.to_csv(index=False).encode('utf-8'),
            file_name=f'gsc_results_final_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
            mime='text/csv'
        )
    else:
        st.error("è¯·å…ˆä¸Šä¼  JSON å¯†é’¥ å’Œ URL åˆ—è¡¨ï¼Œå¹¶è¾“å…¥ GSC å±æ€§ç½‘å€")

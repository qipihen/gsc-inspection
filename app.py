import streamlit as st
import pandas as pd
import re
from datetime import datetime
from google.oauth2 import service_account
from googleapiclient.discovery import build

# ------------------------- GSC API è°ƒç”¨ -------------------------
def inspect_url(sa_file_dict, site_url, url):
    """è°ƒç”¨ Google Search Console API æ£€æµ‹ URL çŠ¶æ€"""
    SCOPES = ['https://www.googleapis.com/auth/webmasters']
    creds = service_account.Credentials.from_service_account_info(
        sa_file_dict, scopes=SCOPES)
    service = build('searchconsole', 'v1', credentials=creds)

    try:
        request = {
            'inspectionUrl': url,
            'siteUrl': site_url
        }
        response = service.urlInspection().index().inspect(body=request).execute()
        status = response['inspectionResult']['indexStatusResult']['coverageState']
        return status
    except Exception as e:
        return f"Error: {e}"

# ------------------------- æ–‡ä»¶è¯»å– + URL åˆ—è¯†åˆ« -------------------------
def load_url_file(file):
    """è‡ªåŠ¨è¯»å– Excel / CSV / TXTï¼Œå¹¶è¯†åˆ« URL åˆ—ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘"""
    filename = file.name.lower()

    # 1. è¯»å–
    if filename.endswith(".xlsx") or filename.endswith(".xls"):
        df = pd.read_excel(file)
    elif filename.endswith(".csv"):
        df = pd.read_csv(file, encoding="utf-8-sig")
    elif filename.endswith(".txt"):
        df = pd.read_csv(file, header=None, names=["url"], encoding="utf-8-sig")
    else:
        st.error("âŒ æ–‡ä»¶ç±»å‹ä¸æ”¯æŒï¼Œè¯·ä¸Šä¼  CSV / TXT / XLSX")
        return None

    # 2. æ¸…æ´—åˆ—å
    df.columns = df.columns.str.strip().str.lower()

    # 3. æ‰¾ URL åˆ—
    url_col = None
    for col in df.columns:
        sample_values = df[col].dropna().astype(str).head(10).tolist()
        if any(re.match(r'^https?://', v.strip()) for v in sample_values):
            url_col = col
            break

    if url_col is None:
        st.error("âŒ æœªæ£€æµ‹åˆ° URL åˆ—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹")
        return None

    # 4. æ•´ç†æˆç»Ÿä¸€æ ¼å¼
    if "status" in df.columns:
        df = df[[url_col, "status"]].rename(columns={url_col: "url"})
    else:
        df = df[[url_col]].rename(columns={url_col: "url"})

    return df

# ------------------------- Streamlit App ä¸»ä½“ -------------------------
st.set_page_config(page_title="GSC URL æ‰¹é‡æ£€æµ‹å·¥å…·", layout="wide")
st.title("ğŸš€ GSC URL Inspection æ‰¹é‡æ£€æµ‹å·¥å…·ï¼ˆäº‘ç«¯å…¨åŠŸèƒ½ç‰ˆï¼‰")

# åˆå§‹åŒ–ç»“æœå­˜å‚¨
if "results" not in st.session_state:
    st.session_state.results = []

# ä¸Šä¼  GSC å‡­è¯
uploaded_json = st.file_uploader("ğŸ“‚ ä¸Šä¼  Google Service Account JSON æ–‡ä»¶", type=["json"])

# ä¸Šä¼  URL æ–‡ä»¶
uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼  URL åˆ—è¡¨ / æ–­ç‚¹ç»­è·‘æ–‡ä»¶ï¼ˆCSV/TXT/XLSXï¼‰", type=["csv", "txt", "xlsx"])

# è¾“å…¥å±æ€§ URL
site_url = st.text_input("ğŸŒ GSC å±æ€§ç½‘å€ï¼ˆä¸ Search Console ä¸­ä¸€è‡´ï¼‰", "https://www.example.com/")

# å¼€å§‹æ£€æµ‹æŒ‰é’®
if st.button("ğŸš€ å¼€å§‹æ£€æµ‹"):
    if uploaded_json and uploaded_file and site_url:
        # è¯»å– JSON æˆå­—å…¸
        try:
            sa_dict = pd.read_json(uploaded_json, typ='series').to_dict()
        except Exception:
            st.error("âŒ JSON æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œè¯·ç¡®è®¤ä¸Šä¼ çš„æ˜¯ Google Service Account å‡­è¯")
            st.stop()

        # è¯»å– URL åˆ—è¡¨
        df_input = load_url_file(uploaded_file)
        if df_input is None:
            st.stop()

        # åˆ¤æ–­æ–­ç‚¹ç»­è·‘æ¨¡å¼
        if "status" in df_input.columns:
            done_count = df_input["status"].notna().sum()
            st.info(f"ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰ {done_count} æ¡ç»“æœï¼Œå°†è·³è¿‡è¿™äº› URL")
            urls_to_check = df_input[df_input["status"].isna()]["url"].tolist()
            st.session_state.results = df_input[df_input["status"].notna()].to_dict("records")
        else:
            urls_to_check = df_input["url"].tolist()

        total_urls = len(urls_to_check)
        st.write(f"ğŸ“Œ æœ¬æ¬¡éœ€æ£€æµ‹ {total_urls} æ¡ URL")

        if total_urls == 0:
            st.warning("æ²¡æœ‰éœ€è¦æ£€æµ‹çš„ URLï¼Œå¯èƒ½æ–‡ä»¶é‡Œéƒ½å·²å®Œæˆã€‚")
            st.stop()

        # è¿›åº¦æ˜¾ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()

        # å¼€å§‹å¾ªç¯æ£€æµ‹
        for idx, url in enumerate(urls_to_check):
            status = inspect_url(sa_dict, site_url, url)
            st.session_state.results.append({
                "url": url,
                "status": status
            })

            # æ›´æ–°è¿›åº¦
            progress_bar.progress((idx + 1) / total_urls)
            status_text.text(f"{idx+1}/{total_urls} å·²å®Œæˆ: {url} â†’ {status}")

            # å®æ—¶ä¸‹è½½æŒ‰é’®ï¼ˆä¸­é€”ä¹Ÿå¯ä¿å­˜ï¼‰
            temp_df = pd.DataFrame(st.session_state.results)
            st.download_button(
                label="â¬‡ ä¸‹è½½å½“å‰è¿›åº¦ CSVï¼ˆå¯æ–­ç‚¹ç»­è·‘ï¼‰",
                data=temp_df.to_csv(index=False).encode('utf-8'),
                file_name="gsc_results_partial.csv",
                mime="text/csv"
            )

        # å…¨éƒ¨å®Œæˆ
        st.success("âœ… æ£€æµ‹å®Œæˆï¼")
        final_df = pd.DataFrame(st.session_state.results)
        st.download_button(
            label="â¬‡ ä¸‹è½½æœ€ç»ˆå®Œæ•´ç»“æœ CSV",
            data=final_df.to_csv(index=False).encode('utf-8'),
            file_name=f'gsc_results_final_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
            mime='text/csv'
        )

    else:
        st.error("è¯·ä¸Šä¼  JSON å‡­è¯æ–‡ä»¶ã€URL æ–‡ä»¶ï¼Œå¹¶è¾“å…¥ GSC å±æ€§ç½‘å€")

import streamlit as st
import pandas as pd
import re
from datetime import datetime
from google.oauth2 import service_account
from googleapiclient.discovery import build

# =====================================================
# ã€1ã€‘è°ƒç”¨ GSC API æ£€æµ‹ URL çŠ¶æ€
# =====================================================
def inspect_url(sa_file_dict, site_url, url):
    """è°ƒç”¨ Google Search Console API æ£€æµ‹ URL çŠ¶æ€"""
    SCOPES = ['https://www.googleapis.com/auth/webmasters']
    creds = service_account.Credentials.from_service_account_info(
        sa_file_dict, scopes=SCOPES)
    service = build('searchconsole', 'v1', credentials=creds)

    try:
        request = {
            'inspectionUrl': url,
            'siteUrl': site_url
        }
        response = service.urlInspection().index().inspect(body=request).execute()
        status = response['inspectionResult']['indexStatusResult']['coverageState']
        return status
    except Exception as e:
        return f"Error: {e}"

# =====================================================
# ã€2ã€‘æ™ºèƒ½ä¸Šä¼ æ–‡ä»¶è¯»å– + è‡ªåŠ¨è¯†åˆ« URL åˆ—
# =====================================================
def load_url_file(file):
    """è‡ªåŠ¨è¯»å– Excel / CSV / TXTï¼Œå¹¶è¯†åˆ« URL åˆ—ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘"""
    filename = file.name.lower()

    # è¯»å–
    if filename.endswith(".xlsx") or filename.endswith(".xls"):
        df = pd.read_excel(file)
    elif filename.endswith(".csv"):
        df = pd.read_csv(file, encoding="utf-8-sig")
    elif filename.endswith(".txt"):
        df = pd.read_csv(file, header=None, names=["url"], encoding="utf-8-sig")
    else:
        st.error("âŒ æ–‡ä»¶ç±»å‹ä¸æ”¯æŒï¼Œè¯·ä¸Šä¼  CSV / TXT / XLSX")
        return None

    # æ¸…æ´—åˆ—å
    df.columns = df.columns.str.strip().str.lower()

    # æ‰¾ URL åˆ—
    url_col = None
    for col in df.columns:
        sample_values = df[col].dropna().astype(str).head(10).tolist()
        if any(re.match(r'^https?://', v.strip()) for v in sample_values):
            url_col = col
            break

    if url_col is None:
        st.error("âŒ æœªæ£€æµ‹åˆ° URL åˆ—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹")
        return None

    # ç»Ÿä¸€æ ¼å¼
    if "status" in df.columns:
        df = df[[url_col, "status"]].rename(columns={url_col: "url"})
    else:
        df = df[[url_col]].rename(columns={url_col: "url"})

    return df

# =====================================================
# ã€3ã€‘Streamlit åº”ç”¨ä¸»ä½“
# =====================================================
st.set_page_config(page_title="GSC URL æ‰¹é‡æ£€æµ‹å·¥å…·", layout="wide")
st.title("ğŸš€ GSC URL Inspection æ‰¹é‡æ£€æµ‹å·¥å…·ï¼ˆäº‘ç«¯æ— å†²çªä¿®æ­£ç‰ˆï¼‰")

if "results" not in st.session_state:
    st.session_state.results = []

# ä¸Šä¼  Google Service Account JSON
uploaded_json = st.file_uploader("ğŸ“‚ ä¸Šä¼  Google Service Account JSON æ–‡ä»¶", type=["json"])

# ä¸Šä¼  URL æ–‡ä»¶
uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼  URL åˆ—è¡¨ / ä¸Šæ¬¡è¿›åº¦æ–‡ä»¶ï¼ˆCSV/TXT/XLSXï¼‰", type=["csv", "txt", "xlsx"])

# è¾“å…¥ GSC å±æ€§ç½‘å€
site_url = st.text_input("ğŸŒ GSC å±æ€§ç½‘å€ï¼ˆä¸ GSC ä¸­å®Œå…¨ä¸€è‡´ï¼‰", "https://www.example.com/")

# =====================================================
# ã€4ã€‘å¼€å§‹æ£€æµ‹æŒ‰é’®
# =====================================================
if st.button("ğŸš€ å¼€å§‹æ£€æµ‹"):
    if uploaded_json and uploaded_file and site_url:
        # è¯»å– JSON ä¸ºå­—å…¸
        try:
            sa_dict = pd.read_json(uploaded_json, typ='series').to_dict()
        except Exception:
            st.error("âŒ JSON æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œè¯·ç¡®è®¤ä¸Šä¼ çš„æ˜¯ Google Service Account å‡­è¯")
            st.stop()

        # è¯»å– URL åˆ—è¡¨
        df_input = load_url_file(uploaded_file)
        if df_input is None:
            st.stop()

        # åˆ¤æ–­æ–­ç‚¹ç»­è·‘
        if "status" in df_input.columns:
            done_count = df_input["status"].notna().sum()
            st.info(f"ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰ {done_count} æ¡ç»“æœï¼Œå°†è·³è¿‡è¿™äº› URL")
            urls_to_check = df_input[df_input["status"].isna()]["url"].tolist()
            st.session_state.results = df_input[df_input["status"].notna()].to_dict("records")
        else:
            urls_to_check = df_input["url"].tolist()

        total_urls = len(urls_to_check)
        st.write(f"ğŸ“Œ æœ¬æ¬¡éœ€æ£€æµ‹ {total_urls} æ¡ URL")

        if total_urls == 0:
            st.warning("æ²¡æœ‰éœ€è¦æ£€æµ‹çš„ URLï¼Œå¯èƒ½æ–‡ä»¶ä¸­å…¨éƒ¨å·²å®Œæˆ")
            st.stop()

        # å¾ªç¯å‰å…ˆæä¾›ä¸€ä¸ªå®æ—¶ä¸‹è½½æŒ‰é’®ï¼ˆéšæ—¶å¯¼å‡ºå½“å‰ resultsï¼‰
        st.download_button(
            label="â¬‡ ä¸‹è½½å½“å‰è¿›åº¦ CSVï¼ˆå¯æ–­ç‚¹ç»­è·‘ï¼‰",
            data=pd.DataFrame(st.session_state.results).to_csv(index=False).encode('utf-8'),
            file_name="gsc_results_partial.csv",
            mime="text/csv",
            key="partial_before_loop"
        )

        # è¿›åº¦æ¡ + çŠ¶æ€æ˜¾ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()

        # å¾ªç¯æ£€æµ‹
        for idx, url in enumerate(urls_to_check):
            status = inspect_url(sa_dict, site_url, url)
            st.session_state.results.append({
                "url": url,
                "status": status
            })

            progress_bar.progress((idx + 1) / total_urls)
            status_text.text(f"{idx+1}/{total_urls} å·²å®Œæˆ: {url} â†’ {status}")

        # æ£€æµ‹å®Œæˆåè¾“å‡ºæœ€ç»ˆä¸‹è½½æŒ‰é’®
        st.success("âœ… æ£€æµ‹å®Œæˆï¼")
        final_df = pd.DataFrame(st.session_state.results)
        st.download_button(
            label="â¬‡ ä¸‹è½½æœ€ç»ˆå®Œæ•´ç»“æœ CSV",
            data=final_df.to_csv(index=False).encode('utf-8'),
            file_name=f'gsc_results_final_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
            mime='text/csv',
            key="final_download"
        )

    else:
        st.error("âŒ è¯·ä¸Šä¼  JSON å‡­è¯æ–‡ä»¶ã€URL æ–‡ä»¶ï¼Œå¹¶è¾“å…¥ GSC å±æ€§ç½‘å€")
